
I want to convert the current testing environment into a workflow with snakemake
I need to do the current pipelining process and put that into one or a few rules depending on some considerations with grid searches
 - E.g. the PWM creation can be testing with more or less top activating samples
 - The training of different SAEs and collecting activations for those SAEs with all of the datasets can be another consideration
Because the deterministic dataset is only used for collecting activations, this is another thing that can come later
I also have to make sure with snakemake that gpu access is facile - I think this should be fine as I have things set up already
Let's figure out how much I want to break up things by just walking through the example.ipynb


Important file paths should be placed in the config and this can be on a per sample basis, which should be a wildcard throughout:
- config.yaml
  - the base directory can also be taken from the config file
  - i think this is important because we can use scratch
  - If no base file path is provided (None), the base directory will default to just the regular path
  - We can have this consideration written out with python in the main Snakefile
  - Other variables can be taken from the config.yaml file as well
  
- Current scripts in the /scripts/ directory can go in /scripts/main_pipeline/
- We will use the bpnetlite conda environment that is already configured to run all of this
- We won't save the training data or deterministic peak generators -> i don't think that's necessary as the det peak gen is the only thing we would
  - need to save and that's created deterministically... it's also created extremely fast. Along with this the only reason we wouldn't do that
  - is so that we didn't have to consider the other packages that would needed to be loaded, but since we're using a consistent env this is fine
- We will have the following rules:

one rule to run the entire pipeline. For the following reasons:
the pipeline comes in a few parts
1. training data creation (with a non-linear creation of the deterministic dataset for collecting activations)
  - not much testing to be done here to be honest, i think this can stay the same for each sample
2. instantiating and training the SAEs
  - the hyperparams here are: [center_len, expansion_factor, topk_fraction, training hyperparams]
3. loading saved models and collecting activations (and plotting)
  - there are no hyperparams here
  
If i split up the pipeline into sections, it would require further of saving of objects (like the trainer which is passed from 2 to 3)
If I keep everything consistent and my grid searches only span the following then it's okay and computationally inexpsive to run everything at once
- grid search parameters:
  - expansion_factor, topk_fraction, center_len




# Prepare training data and other information
import numpy
import os
import torch
from bpnetlite.io import PeakGenerator
from scripts import deterministic_data_loaders as ddl
from scripts import models as mds
from scripts import SAE_trainer as st
from scripts import save_activations as sa
from scripts import plot_activation_frequencies as nf

training_data = PeakGenerator(
    peaks = peaks_bed,
    negatives = negatives_bed,
    sequences = seqs,
    signals = [signal_plus, signal_minus],
    controls = [ctl_plus, ctl_minus],
    chroms = None,
    in_window = 2114,
    out_window = 1000,
    max_jitter = 128,
    negative_ratio = 0.33,
    reverse_complement = True,
    shuffle = True,
    min_counts = None,
    max_counts = None,
    summits = False,
    exclusion_lists = None,
    random_state = 12345,
    pin_memory = True,
    num_workers = 0,
    batch_size = 64,
    verbose = True
)

# Initialize the dataloader to pass all peaks through SAEs and capture activations in order of bed file
sae_testing_data = ddl.DeterministicPeakGenerator(
    peaks=[peaks_bed, negatives_bed],
    sequences=seqs,
    signals=[signal_plus, signal_minus],
    chroms=None,
    in_window=2114,
    out_window=1000,
    pin_memory=True,
    batch_size=64,
    verbose=True
)

# Instantiate the trainer object
trainer = st.SAETrainer(model_path=model_path, device="cuda", center_len=center_length)

# Train an SAE for each layer on all training data with given hyperparameters
trainer.train_all(
    train_loader=training_data,
    sae_cls=mds.SAETopK,
    sae_kwargs={"latent_multiplier": expansion_factor, "k_fraction": topk_fraction},
    save_dir=f'{out_dir}/models',
    logs_dir=f'{out_dir}/models/logs',
    epochs=epochs,
    inner_bs=inner_bs,
    lr=lr,
    log_every=50,
)

# Load the trained SAEs from disk written by trainer.train_all()
sae_models = mds.load_saes_from_dir(save_dir=f'{out_dir}/models', layers=trainer.layers, device=trainer.device)

# Run Top-K collection over deterministic data loader
meta = sa.collect_topk_indices_to_disk_from_trainer(
    trainer=trainer,
    sae_models=sae_models,
    loader=sae_testing_data,
    out_dir=f'{out_dir}/activations',
)

# Plot node activation frequencies
nf.plot_node_activation_frequencies(
	num_layers = len(trainer.layers), 
	latent_dim = int(64*expansion_factor), 
	data_dir = f'{out_dir}/activations'
)

